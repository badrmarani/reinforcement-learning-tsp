{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a48628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af67f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(ntours, embed_size):\n",
    "    t = torch.arange(0, ntours, 1, dtype=dtype).unsqueeze(1)\n",
    "    fi = torch.exp(\n",
    "        -torch.arange(0, embed_size, 2) * torch.log(torch.tensor(10_000)) / embed_size\n",
    "    )\n",
    "\n",
    "    pe = torch.zeros(ntours, embed_size)\n",
    "    pe[:, 0::2] = torch.sin(fi*t)\n",
    "    pe[:, 1::2] = torch.cos(fi*t)\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dc37317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSP(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int,\n",
    "        nnodes:int,\n",
    "        xxx:int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        # ENCODER\n",
    "        self.embed = nn.Linear(2, embed_size)\n",
    "        self.encoder = Encoder(nheads=nheads, embed_size=embed_size, ff_size=ff_size, nlayers=nlayers)\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder = Decoder()\n",
    "        self.Wk = nn.Linear(embed_size, xxx * embed_size, bias=False)\n",
    "        self.Wv = nn.Linear(embed_size, xxx * embed_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.embed(x)\n",
    "        h = self.encoder(h)\n",
    "\n",
    "        K, V = self.Wk(h), self.Wv(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "2c85de9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwhat torch.Size([3, 1, 8])\n",
      "torch.Size([12, 1, 1]) torch.Size([12, 1, 2])\n",
      "torch.Size([12, 1, 11]) torch.Size([12, 11, 2])\n",
      "1 tensor([[-0.3766, -0.9537,  0.8644,  0.4186, -0.9731, -1.4053,  0.9279,  1.4978],\n",
      "        [ 2.0470,  0.2075, -1.1381,  0.1470,  0.1432,  0.3725, -0.2944, -1.4846],\n",
      "        [ 0.7489, -1.1418, -1.4837,  1.5960,  0.6947,  0.5540, -0.7376, -0.2305]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "2 tensor([[ 0.0747, -0.5848,  0.4539,  0.9453, -1.6970, -1.2422,  0.9055,  1.1445],\n",
      "        [ 1.8162,  0.4295, -1.6982,  0.0605, -0.0894,  0.5995,  0.0051, -1.1233],\n",
      "        [ 0.8660, -1.3136, -1.5051,  1.3808,  1.0478, -0.5029, -0.0140,  0.0411]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def pos_encoding(ntours, embed_size):\n",
    "    t = torch.arange(0, ntours, 1, dtype=dtype).unsqueeze(1)\n",
    "    fi = torch.exp(\n",
    "        -torch.arange(0, embed_size, 2) * torch.log(torch.tensor(10_000)) / embed_size\n",
    "    )\n",
    "\n",
    "    pe = torch.zeros(ntours, embed_size)\n",
    "    pe[:, 0::2] = torch.sin(fi*t)\n",
    "    pe[:, 1::2] = torch.cos(fi*t)\n",
    "    return pe\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int  \n",
    "    ) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        assert not embed_size % nheads, \"The embedding size has to be a multiple of the number of heads.\"\n",
    "\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.mha = nn.ModuleList(nn.MultiheadAttention(embed_size, nheads, batch_first=True, bias=False) for _ in range(self.nlayers))\n",
    "\n",
    "        self.ff1 = nn.ModuleList(nn.Linear(embed_size, ff_size) for _ in range(self.nlayers))\n",
    "        self.ff2 = nn.ModuleList(nn.Linear(ff_size, embed_size) for _ in range(self.nlayers))\n",
    "\n",
    "        self.bn = nn.ModuleList(nn.BatchNorm1d(embed_size) for _ in range(self.nlayers))\n",
    "        \n",
    "    def forward(self, h):\n",
    "        hprev = torch.empty_like(h)\n",
    "        for i in range(self.nlayers):\n",
    "            hprev.copy_(h)\n",
    "            h, _ = self.mha[i](h,h,h)\n",
    "            h = h + hprev\n",
    "\n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "            h = self.bn[i](h)\n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "\n",
    "            hprev.copy_(h)\n",
    "            h = self.ff2[i](torch.relu(self.ff1[i](h)))\n",
    "            h = h + hprev\n",
    "            \n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "            h = self.bn[i](h)\n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "        return h\n",
    "\n",
    "class MHA_(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int,\n",
    "    ) -> None:\n",
    "        super(MHA_, self).__init__()\n",
    "        assert not embed_size % nheads, \"The embedding size has to be a divisible by the number of heads.\"\n",
    "        self.d_k = embed_size // nheads\n",
    "        self.nheads = nheads\n",
    "        self.embed_size = embed_size\n",
    "        self.ff_size = ff_size\n",
    "        self.nlayers = nlayers\n",
    "    \n",
    "    def _attention(self, query, key, value, mask=None, clip=None):\n",
    "        attn = torch.bmm(query, key.transpose(1,2)) / self.d_k ** .5\n",
    "        if mask is not None:\n",
    "            if self.nheads > 1:\n",
    "                mask = torch.repeat_interleave(mask, repeats=self.nheads, dim=0)\n",
    "\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn = attn.masked_fill(mask, float(\"-1e9\"))\n",
    "        \n",
    "        if clip is not None:\n",
    "            attn = clip * torch.tanh(attn)\n",
    "        \n",
    "        p_attn = attn.softmax(dim=-1)\n",
    "        print(p_attn.size(), value.size())\n",
    "        return torch.bmm(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, clip=None):\n",
    "        nbatchs = query.size(0)\n",
    "        if self.nheads > 1:\n",
    "            query, key, value = [\n",
    "                x.transpose(1,2).contiguous().view(nbatchs*self.nheads, self.d_k, nnd).transpose(1,2).contiguous()\n",
    "                for nnd, x in zip((query.size(1), key.size(1), value.size(1)), (query, key, value))\n",
    "            ]\n",
    "\n",
    "        x, _ = self._attention(query, key, value, mask=mask)\n",
    "\n",
    "        x = x.view(nbatchs, -1, self.d_k * self.nheads) if self.nheads > 1 else x\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "class AutoregressiveDecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.ModuleList(nn.Linear(embed_size, embed_size) for _ in range(11))\n",
    "        self.bn = nn.ModuleList(nn.LayerNorm(embed_size) for _ in range(3))\n",
    "\n",
    "        self.kprev = None\n",
    "        self.vprev = None\n",
    "\n",
    "        self.mha = MHA_(nheads, embed_size, ff_size, nlayers)\n",
    "\n",
    "    def forward(self, ht, key, value, mask):\n",
    "        nbatchs = ht.size(0)\n",
    "        print(\"wwhat\", ht.size())\n",
    "\n",
    "        # STEP (2)\n",
    "        q = self.lin[0](ht)\n",
    "        k = self.lin[1](ht)\n",
    "        v = self.lin[2](ht)\n",
    "\n",
    "        if self.kprev is None:\n",
    "            self.kprev = k\n",
    "            self.vprev = v\n",
    "        else:\n",
    "            self.kprev = torch.cat([self.kprev, k], dim=1)\n",
    "            self.vprev = torch.cat([self.vprev, v], dim=1)\n",
    "        \n",
    "        ht += self.lin[3](self.mha(q, self.kprev, self.vprev, mask=None))\n",
    "        ht = self.bn[0](ht.squeeze()).view(nbatchs, 1, -1)\n",
    "\n",
    "        # STEP (3)\n",
    "        q = self.lin[4](ht)\n",
    "        ht += self.lin[5](self.mha(q, key, value, mask))\n",
    "        ht = self.bn[1](ht.squeeze()).view(nbatchs, 1, -1)\n",
    "\n",
    "        # STEP (4)\n",
    "        ht += self.lin[6](torch.relu(self.lin[7](ht)))\n",
    "        ht = self.bn[2](ht.squeeze(1))\n",
    "        return ht\n",
    "\n",
    "nheads = 4\n",
    "nnodes = 11\n",
    "embed_size = 8\n",
    "ff_size = 21\n",
    "nlayers = 3\n",
    "nbatchs = 3\n",
    "xxx = 5\n",
    "\n",
    "# enc = Encoder(nheads, nnodes, embed_size, ff_size, nlayers)\n",
    "x = torch.randn(nbatchs, 1, embed_size)\n",
    "k = torch.randn(nbatchs, nnodes, embed_size)\n",
    "v = torch.randn(nbatchs, nnodes, embed_size)\n",
    "mask = torch.ones((nbatchs, nnodes))\n",
    "\n",
    "tsp = AutoregressiveDecoderLayer(nheads, embed_size, ff_size, nlayers)\n",
    "print(\"1\", tsp(x, k, v, mask))\n",
    "# m = MHA_(nheads, embed_size, ff_size, nlayers, nnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "8313398e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mask tensor can take 0 and 1 values only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [562], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# tsp = AutoregressiveDecoderLayer(nheads, embed_size, ff_size, nlayers)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m m \u001b[39m=\u001b[39m MHA_(nheads, embed_size, ff_size, nheads, nnodes)\n\u001b[1;32m---> 18\u001b[0m m(q, k, v, mask)\u001b[39m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [561], line 96\u001b[0m, in \u001b[0;36mMHA_.forward\u001b[1;34m(self, query, key, value, mask, clip)\u001b[0m\n\u001b[0;32m     89\u001b[0m nbatchs \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     91\u001b[0m query, key, value \u001b[39m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     x\u001b[39m.\u001b[39mview(nbatchs\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnheads, nnd, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m nnd, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m((\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnodes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnodes), (query, key, value))\n\u001b[0;32m     94\u001b[0m ]\n\u001b[1;32m---> 96\u001b[0m x, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attention(query, key, value, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m     97\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(nbatchs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnheads) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnheads \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m x\n\u001b[0;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn [561], line 79\u001b[0m, in \u001b[0;36mMHA_._attention\u001b[1;34m(self, query, key, value, mask, clip)\u001b[0m\n\u001b[0;32m     76\u001b[0m         mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(mask, repeats\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnheads, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     78\u001b[0m     mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m     attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39;49mmasked_fill(mask, \u001b[39mfloat\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m-1e9\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m clip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     attn \u001b[39m=\u001b[39m clip \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtanh(attn)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Mask tensor can take 0 and 1 values only"
     ]
    }
   ],
   "source": [
    "nheads = 8\n",
    "nnodes = 10\n",
    "embed_size = 128\n",
    "ff_size = 21\n",
    "nlayers = 3\n",
    "nbatchs = 3\n",
    "xxx = 5\n",
    "\n",
    "# enc = Encoder(nheads, nnodes, embed_size, ff_size, nlayers)\n",
    "q = torch.randn(nbatchs, 1, embed_size)\n",
    "k = torch.randn(nbatchs, nnodes, embed_size)\n",
    "v = torch.randn(nbatchs, nnodes, embed_size)\n",
    "mask = torch.randn(nbatchs, nnodes)\n",
    "\n",
    "# tsp = AutoregressiveDecoderLayer(nheads, embed_size, ff_size, nlayers)\n",
    "\n",
    "m = MHA_(nheads, embed_size, ff_size, nheads, nnodes)\n",
    "m(q, k, v, mask).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
