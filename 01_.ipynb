{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a48628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af67f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(ntours, embed_size):\n",
    "    t = torch.arange(0, ntours, 1, dtype=dtype).unsqueeze(1)\n",
    "    fi = torch.exp(\n",
    "        -torch.arange(0, embed_size, 2) * torch.log(torch.tensor(10_000)) / embed_size\n",
    "    )\n",
    "\n",
    "    pe = torch.zeros(ntours, embed_size)\n",
    "    pe[:, 0::2] = torch.sin(fi*t)\n",
    "    pe[:, 1::2] = torch.cos(fi*t)\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dc37317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSP(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int,\n",
    "        nnodes:int,\n",
    "        xxx:int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        # ENCODER\n",
    "        self.embed = nn.Linear(2, embed_size)\n",
    "        self.encoder = Encoder(nheads=nheads, embed_size=embed_size, ff_size=ff_size, nlayers=nlayers)\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder = Decoder()\n",
    "        self.Wk = nn.Linear(embed_size, xxx * embed_size, bias=False)\n",
    "        self.Wv = nn.Linear(embed_size, xxx * embed_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.embed(x)\n",
    "        h = self.encoder(h)\n",
    "\n",
    "        K, V = self.Wk(h), self.Wv(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "2c85de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(ntours, embed_size):\n",
    "    t = torch.arange(0, ntours, 1, dtype=dtype).unsqueeze(1)\n",
    "    fi = torch.exp(\n",
    "        -torch.arange(0, embed_size, 2) * torch.log(torch.tensor(10_000)) / embed_size\n",
    "    )\n",
    "\n",
    "    pe = torch.zeros(ntours, embed_size)\n",
    "    pe[:, 0::2] = torch.sin(fi*t)\n",
    "    pe[:, 1::2] = torch.cos(fi*t)\n",
    "    return pe\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int  \n",
    "    ) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        assert not embed_size % nheads, \"The embedding size has to be a multiple of the number of heads.\"\n",
    "\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.mha = nn.ModuleList(nn.MultiheadAttention(embed_size, nheads, batch_first=True, bias=False) for _ in range(self.nlayers))\n",
    "\n",
    "        self.ff1 = nn.ModuleList(nn.Linear(embed_size, ff_size) for _ in range(self.nlayers))\n",
    "        self.ff2 = nn.ModuleList(nn.Linear(ff_size, embed_size) for _ in range(self.nlayers))\n",
    "\n",
    "        self.bn = nn.ModuleList(nn.BatchNorm1d(embed_size) for _ in range(self.nlayers))\n",
    "        \n",
    "    def forward(self, h):\n",
    "        hprev = torch.empty_like(h)\n",
    "        for i in range(self.nlayers):\n",
    "            hprev.copy_(h)\n",
    "            h, _ = self.mha[i](h,h,h)\n",
    "            h = h + hprev\n",
    "\n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "            h = self.bn[i](h)\n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "\n",
    "            hprev.copy_(h)\n",
    "            h = self.ff2[i](torch.relu(self.ff1[i](h)))\n",
    "            h = h + hprev\n",
    "            \n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "            h = self.bn[i](h)\n",
    "            h = h.permute(0,2,1).contiguous()\n",
    "        return h\n",
    "\n",
    "class MHA_(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int,\n",
    "    ) -> None:\n",
    "        super(MHA_, self).__init__()\n",
    "        assert not embed_size % nheads, \"The embedding size has to be a divisible by the number of heads.\"\n",
    "        self.d_k = embed_size // nheads\n",
    "        self.nheads = nheads\n",
    "        self.embed_size = embed_size\n",
    "        self.ff_size = ff_size\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.ff = nn.ModuleList(nn.Linear(embed_size, embed_size) for _ in range(nlayers))\n",
    "    \n",
    "    def _attention(self, query, key, value, mask=None, clip=None):\n",
    "        attn = torch.matmul(query, key.transpose(-2,-1)) / query.size(-1) ** .5\n",
    "        if mask is not None:\n",
    "            if self.nheads > 1:\n",
    "                mask = torch.repeat_interleave(mask, repeats=self.nheads, dim=0)\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn = attn.masked_fill(mask == 0, -1e-9)\n",
    "\n",
    "        if clip is not None:\n",
    "            attn = clip * torch.tanh(attn)\n",
    "        \n",
    "        p_attn = attn.softmax(dim=-1)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, clip=None):\n",
    "        nbatchs = query.size(0)\n",
    "        \n",
    "        query, key, value = [\n",
    "            x.view(nbatchs, self.nheads, -1, self.d_k)\n",
    "            for x in (query, key, value)\n",
    "        ]\n",
    "\n",
    "        x, attn = self._attention(query, key, value, mask=mask)\n",
    "        x = x.view(nbatchs, -1, self.d_k * self.nheads)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "class AutoregressiveDecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "        nheads:int,\n",
    "        embed_size:int,\n",
    "        ff_size:int,\n",
    "        nlayers:int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.Wq = nn.ModuleList(nn.Linear(embed_size, embed_size) for _ in range(2))\n",
    "        self.Wk = nn.ModuleList(nn.Linear(embed_size, embed_size) for _ in range(2))\n",
    "        self.Wv = nn.ModuleList(nn.Linear(embed_size, embed_size) for _ in range(2))\n",
    "        self.lin1 = nn.Linear(embed_size, embed_size)\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.kprev = None\n",
    "        self.vprev = None\n",
    "\n",
    "        self.mha = MHA_(nheads, embed_size, ff_size, nlayers)\n",
    "    def forward(self, ht, key, value, mask=None):\n",
    "        nbatchs = ht.size(0)\n",
    "\n",
    "        # STEP (2)\n",
    "        q = self.Wq[0](ht)\n",
    "        k = self.Wk[0](ht)\n",
    "        v = self.Wv[0](ht)\n",
    "\n",
    "        if self.kprev is  None:\n",
    "            self.kprev = k\n",
    "            self.vprev = v\n",
    "        else:\n",
    "            self.kprev = torch.cat([self.kprev, k], dim=1)\n",
    "            self.vprev = torch.cat([self.vprev, v], dim=1)\n",
    "        \n",
    "        ht += self.lin1(self.mha(q, self.kprev, self.vprev))\n",
    "        ht = self.bn1(ht.squeeze(1)).view(nbatchs, 1, -1)\n",
    "\n",
    "        # STEP (3)\n",
    "        q = self\n",
    "\n",
    "        print(ht.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "8313398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "nheads = 1\n",
    "nnodes = 10\n",
    "embed_size = 4\n",
    "ff_size = 21\n",
    "nlayers = 3\n",
    "nbatchs = 3\n",
    "xxx = 5\n",
    "\n",
    "# enc = Encoder(nheads, nnodes, embed_size, ff_size, nlayers)\n",
    "x = torch.randn(nbatchs, 1, embed_size)\n",
    "k = torch.randn(nbatchs, 20, embed_size)\n",
    "q = torch.randn(nbatchs, 20, embed_size)\n",
    "v = torch.randn(nbatchs, 20, embed_size)\n",
    "\n",
    "tsp = AutoregressiveDecoderLayer(nheads, embed_size, ff_size, nlayers)\n",
    "tsp(x, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "88db4529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1133,  0.7578, -0.4857, -0.2076]]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.randn(nbatchs, 1, embed_size)\n",
    "# myMHA(x, x, x, nheads)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "285ec4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
